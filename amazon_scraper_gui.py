#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Amazon Product Scraper - GUI Edition
Giao di·ªán ƒë·ªì h·ªça cho scraping Amazon products
G·ªôp t·∫•t c·∫£ t√≠nh nƒÉng v√†o 1 file - CH·ªà S·ª¨ D·ª§NG GUI

Requires: requests, beautifulsoup4, lxml, tkinter
"""

import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox, filedialog
import requests
from bs4 import BeautifulSoup
import json
import re
import time
import random
from urllib.parse import urlparse, urlencode, parse_qs
import threading
import os
from datetime import datetime
import webbrowser
import sys

# ===================================================================
# CORE AMAZON SCRAPER CLASS
# ===================================================================

class AmazonScraper:
    def __init__(self):
        self.session = requests.Session()
        
        # Rotate user agents to avoid detection
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
        
        # Headers to mimic real browser
        self.headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

    def get_random_headers(self):
        """Get random headers to avoid detection"""
        headers = self.headers.copy()
        headers['User-Agent'] = random.choice(self.user_agents)
        return headers

    def validate_amazon_url(self, url):
        """Validate if the URL is an Amazon product URL"""
        parsed_url = urlparse(url)
        amazon_domains = ['amazon.com', 'amazon.co.uk', 'amazon.de', 'amazon.fr', 'amazon.it', 'amazon.es', 'amazon.jp']
        
        if not any(domain in parsed_url.netloc for domain in amazon_domains):
            return False
        
        # Check if it's a product URL (contains /dp/ or /gp/product/)
        if '/dp/' in url or '/gp/product/' in url:
            return True
        
        return False

    def extract_product_info(self, soup):
        """Extract product information from BeautifulSoup object"""
        product_info = {}
        
        try:
            # Product title
            title_selectors = [
                '#productTitle',
                '.product-title',
                'h1.a-size-large',
                'h1#title'
            ]
            
            for selector in title_selectors:
                title_element = soup.select_one(selector)
                if title_element:
                    product_info['title'] = title_element.get_text().strip()
                    break
            
            # ASIN (Amazon Standard Identification Number)
            asin_patterns = [
                r'/dp/([A-Z0-9]{10})',
                r'/gp/product/([A-Z0-9]{10})',
                r'data-asin="([A-Z0-9]{10})"'
            ]
            page_content = str(soup)
            for pattern in asin_patterns:
                asin_match = re.search(pattern, page_content)
                if asin_match:
                    product_info['asin'] = asin_match.group(1)
                    break
            
            # Brand
            brand_selectors = [
                '#bylineInfo',
                '.a-row .a-link-normal[href*="/stores/"]',
                'tr:contains("Brand") td.a-span9',
                '.po-brand .po-break-word',
                '#brand'
            ]
            
            for selector in brand_selectors:
                brand_element = soup.select_one(selector)
                if brand_element:
                    brand_text = brand_element.get_text().strip()
                    if brand_text and not brand_text.lower().startswith('visit'):
                        product_info['brand'] = brand_text.replace('Brand: ', '').replace('Visit the ', '').replace(' Store', '')
                        break
            
            # Product price
            price_selectors = [
                '.a-price-whole',
                '.a-price .a-offscreen',
                '#price_inside_buybox',
                '.a-price-range',
                '#ap_desktop_sns_detail_page .a-price .a-offscreen'
            ]
            
            for selector in price_selectors:
                price_element = soup.select_one(selector)
                if price_element:
                    price_text = price_element.get_text().strip()
                    # Clean price text
                    price_text = re.sub(r'[^\d.,]', '', price_text)
                    product_info['price'] = price_text
                    break
            
            # Product rating
            rating_selectors = [
                '.a-icon-alt',
                '[data-hook="average-star-rating"] .a-icon-alt',
                '.a-star-medium .a-icon-alt'
            ]
            
            for selector in rating_selectors:
                rating_element = soup.select_one(selector)
                if rating_element:
                    rating_text = rating_element.get('alt', '') or rating_element.get_text()
                    rating_match = re.search(r'(\d+\.?\d*)', rating_text)
                    if rating_match:
                        product_info['rating'] = rating_match.group(1)
                        break
            
            # Number of reviews
            review_selectors = [
                '#acrCustomerReviewText',
                '[data-hook="total-review-count"]',
                '.a-link-normal .a-size-base'
            ]
            
            for selector in review_selectors:
                review_element = soup.select_one(selector)
                if review_element:
                    review_text = review_element.get_text().strip()
                    review_match = re.search(r'([\d,]+)', review_text)
                    if review_match:
                        product_info['review_count'] = review_match.group(1)
                        break
            
            # Product images
            img_selectors = [
                '#landingImage',
                '.a-dynamic-image',
                '#imgTagWrapperId img'
            ]
            
            images = []
            for selector in img_selectors:
                img_elements = soup.select(selector)
                for img in img_elements:
                    src = img.get('src') or img.get('data-src')
                    if src and src.startswith('http'):
                        images.append(src)
            
            if images:
                product_info['images'] = list(set(images))  # Remove duplicates
            
            # Product description/features
            feature_selectors = [
                '#feature-bullets ul li',
                '.a-unordered-list .a-list-item',
                '#productDescription p'
            ]
            
            features = []
            for selector in feature_selectors:
                feature_elements = soup.select(selector)
                for feature in feature_elements:
                    text = feature.get_text().strip()
                    if text and len(text) > 10:  # Filter out short/empty text
                        features.append(text)
                if features:  # If we found features, break
                    break
            
            if features:
                product_info['features'] = features[:5]  # Limit to first 5 features
            
            # Availability
            availability_selectors = [
                '#availability span',
                '.a-size-medium.a-color-success',
                '.a-size-medium.a-color-price'
            ]
            
            for selector in availability_selectors:
                avail_element = soup.select_one(selector)
                if avail_element:
                    product_info['availability'] = avail_element.get_text().strip()
                    break
            
            # Technical Specifications / Product Details
            product_info['specifications'] = {}
            
            # Method 1: Technical Details table
            tech_table = soup.select_one('#productDetails_techSpec_section_1')
            if tech_table:
                rows = tech_table.select('tr')
                for row in rows:
                    cols = row.select('td')
                    if len(cols) >= 2:
                        key = cols[0].get_text().strip()
                        value = cols[1].get_text().strip()
                        if key and value:
                            product_info['specifications'][key] = value
            
            # Method 2: Feature bullets for specifications
            detail_bullets = soup.select('#feature-bullets ul li, .a-unordered-list.a-nostyle li')
            for bullet in detail_bullets:
                text = bullet.get_text().strip()
                if ':' in text and len(text) < 200:  # Likely a specification
                    parts = text.split(':', 1)
                    if len(parts) == 2:
                        key = parts[0].strip()
                        value = parts[1].strip()
                        if key and value and not key.lower().startswith('make sure'):
                            product_info['specifications'][key] = value
            
            # Method 3: Product Overview section
            overview_section = soup.select_one('#poExpander')
            if overview_section:
                overview_rows = overview_section.select('.po-display-name')
                overview_values = overview_section.select('.po-break-word')
                for i, row in enumerate(overview_rows):
                    if i < len(overview_values):
                        key = row.get_text().strip()
                        value = overview_values[i].get_text().strip()
                        if key and value:
                            product_info['specifications'][key] = value
            
            # Method 4: Additional Information table
            additional_info = soup.select('#productDetails_detailBullets_sections1 tr')
            for row in additional_info:
                th = row.select_one('th')
                td = row.select_one('td')
                if th and td:
                    key = th.get_text().strip()
                    value = td.get_text().strip()
                    if key and value:
                        product_info['specifications'][key] = value
            
            # Extract specific important fields from specifications
            specs = product_info.get('specifications', {})
            
            # Color
            color_keys = ['Color', 'Colour', 'Color Name', 'Item Color']
            for key in color_keys:
                if key in specs:
                    product_info['color'] = specs[key]
                    break
            
            # Material
            material_keys = ['Material', 'Materials', 'Item Material', 'Frame Material', 'Fabric Type']
            for key in material_keys:
                if key in specs:
                    product_info['material'] = specs[key]
                    break
            
            # Size/Dimensions
            size_keys = ['Size', 'Dimensions', 'Item Dimensions', 'Package Dimensions', 'Product Dimensions']
            for key in size_keys:
                if key in specs:
                    product_info['dimensions'] = specs[key]
                    break
            
            # Weight
            weight_keys = ['Weight', 'Item Weight', 'Package Weight', 'Shipping Weight']
            for key in weight_keys:
                if key in specs:
                    product_info['weight'] = specs[key]
                    break
            
            # Model Number
            model_keys = ['Model Number', 'Model', 'Item model number', 'Part Number']
            for key in model_keys:
                if key in specs:
                    product_info['model_number'] = specs[key]
                    break
            
            # Department/Category
            category_selectors = [
                '#wayfinding-breadcrumbs_feature_div a',
                '.a-breadcrumb a',
                '[data-hook="breadcrumb"] a'
            ]
            
            categories = []
            for selector in category_selectors:
                category_links = soup.select(selector)
                for link in category_links:
                    cat_text = link.get_text().strip()
                    if cat_text and cat_text not in categories:
                        categories.append(cat_text)
            
            if categories:
                product_info['categories'] = categories
                product_info['primary_category'] = categories[-1] if categories else None
            
            # Best Sellers Rank
            rank_element = soup.select_one('#SalesRank, .a-icon-badge')
            if rank_element:
                rank_text = rank_element.get_text().strip()
                if 'Best Sellers Rank' in rank_text or '#' in rank_text:
                    product_info['bestsellers_rank'] = rank_text
            
            # Prime eligibility
            prime_elements = soup.select('.a-icon-prime, [data-csa-c-content-id="prime-sash"]')
            if prime_elements:
                product_info['prime_eligible'] = True
            else:
                product_info['prime_eligible'] = False
            
            # Product description (detailed)
            description_selectors = [
                '#productDescription p',
                '#aplus_feature_div',
                '.a-section.a-spacing-medium.apm-A1sMoFEeI'
            ]
            
            descriptions = []
            for selector in description_selectors:
                desc_elements = soup.select(selector)
                for desc in desc_elements:
                    desc_text = desc.get_text().strip()
                    if desc_text and len(desc_text) > 20 and desc_text not in descriptions:
                        descriptions.append(desc_text)
            
            if descriptions:
                product_info['detailed_description'] = descriptions
            
            # Variations (size, color options)
            variations = {}
            
            # Color variations
            color_swatches = soup.select('.imgSwatch, .a-button-text .a-size-base')
            if color_swatches:
                color_options = []
                for swatch in color_swatches:
                    color_name = swatch.get('title') or swatch.get_text().strip()
                    if color_name and color_name not in color_options:
                        color_options.append(color_name)
                if color_options:
                    variations['colors'] = color_options
            
            # Size variations
            size_select = soup.select('#native_dropdown_selected_size_name option, .a-size-base.a-color-base')
            if size_select:
                size_options = []
                for size in size_select:
                    size_name = size.get_text().strip()
                    if size_name and size_name not in ['Select', 'Choose', ''] and size_name not in size_options:
                        size_options.append(size_name)
                if size_options:
                    variations['sizes'] = size_options
            
            if variations:
                product_info['variations'] = variations
            
            # Shipping information
            shipping_element = soup.select_one('#deliveryBlockMessage, .a-spacing-top-base .a-color-price')
            if shipping_element:
                shipping_text = shipping_element.get_text().strip()
                if 'delivery' in shipping_text.lower() or 'shipping' in shipping_text.lower():
                    product_info['shipping_info'] = shipping_text
            
            # Seller information
            seller_element = soup.select_one('#sellerProfileTriggerId, .a-size-small.mbcMerchantName')
            if seller_element:
                seller_text = seller_element.get_text().strip()
                if seller_text:
                    product_info['seller'] = seller_text
            
        except Exception as e:
            print(f"Error extracting product info: {e}")
        
        return product_info

    def scrape_product(self, url):
        """Main method to scrape product from Amazon URL"""
        
        # Validate URL
        if not self.validate_amazon_url(url):
            return {
                'error': 'Invalid Amazon product URL. Please provide a valid Amazon product link.'
            }
        
        try:
            # Add random delay to avoid rate limiting
            time.sleep(random.uniform(1, 3))
            
            # Make request with random headers
            response = self.session.get(url, headers=self.get_random_headers())
            response.raise_for_status()
            
            # Parse HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract product information
            product_info = self.extract_product_info(soup)
            
            # Add URL and timestamp
            product_info['url'] = url
            product_info['scraped_at'] = time.strftime('%Y-%m-%d %H:%M:%S')
            
            return product_info
            
        except requests.exceptions.RequestException as e:
            return {
                'error': f'Network error: {str(e)}'
            }
        except Exception as e:
            return {
                'error': f'Scraping error: {str(e)}'
            }

# ===================================================================
# AMAZON SEARCH SCRAPER CLASS
# ===================================================================

class AmazonSearchScraper:
    def __init__(self):
        self.session = requests.Session()
        self.base_scraper = AmazonScraper()
        
        # User agents for rotation
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
        
        self.headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

    def get_random_headers(self):
        """Get random headers to avoid detection"""
        headers = self.headers.copy()
        headers['User-Agent'] = random.choice(self.user_agents)
        return headers

    def validate_search_url(self, url):
        """Validate if the URL is an Amazon search URL"""
        parsed_url = urlparse(url)
        amazon_domains = ['amazon.com', 'amazon.co.uk', 'amazon.de', 'amazon.fr', 'amazon.it', 'amazon.es', 'amazon.jp']
        
        if not any(domain in parsed_url.netloc for domain in amazon_domains):
            return False
        
        # Check if it's a search URL (contains /s? or has 'k=' parameter)
        if '/s?' in url or 'k=' in url:
            return True
        
        return False

    def build_page_url(self, base_url, page_num):
        """Build URL for specific page number"""
        if page_num == 1:
            return base_url
        
        # Parse the URL and add page parameter
        parsed_url = urlparse(base_url)
        query_params = parse_qs(parsed_url.query)
        
        # Add page parameter
        query_params['page'] = [str(page_num)]
        
        # Reconstruct URL
        new_query = urlencode(query_params, doseq=True)
        new_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query}"
        
        return new_url

    def extract_product_links(self, soup):
        """Extract product links from search results page"""
        product_links = []
        
        # Various selectors for product links
        selectors = [
            'h2.a-size-mini a',
            '.s-result-item h3 a',
            '[data-component-type="s-search-result"] h3 a',
            '.s-product-image-container a',
            'a.a-link-normal.s-underline-text'
        ]
        
        for selector in selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href')
                if href and ('/dp/' in href or '/gp/product/' in href):
                    # Convert relative URL to absolute
                    if href.startswith('/'):
                        href = 'https://www.amazon.com' + href
                    product_links.append(href)
        
        # Remove duplicates while preserving order
        seen = set()
        unique_links = []
        for link in product_links:
            # Clean URL (remove tracking parameters after dp/ASIN)
            clean_link = re.sub(r'(/dp/[A-Z0-9]{10}).*', r'\1', link)
            if clean_link not in seen:
                seen.add(clean_link)
                unique_links.append(clean_link)
        
        return unique_links

    def scrape_search_results(self, search_url, max_pages=1, progress_callback=None):
        """Scrape products from Amazon search results"""
        
        if not self.validate_search_url(search_url):
            return {
                'error': 'Invalid Amazon search URL. Please provide a valid Amazon search link.'
            }
        
        all_products = []
        total_scraped = 0
        
        try:
            for page_num in range(1, max_pages + 1):
                if progress_callback:
                    progress_callback(f"ƒêang scrape trang {page_num}/{max_pages}...")
                
                # Build URL for current page
                page_url = self.build_page_url(search_url, page_num)
                
                # Add delay between pages
                if page_num > 1:
                    time.sleep(random.uniform(2, 4))
                
                # Get search results page
                response = self.session.get(page_url, headers=self.get_random_headers())
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Extract product links
                product_links = self.extract_product_links(soup)
                
                if not product_links:
                    if progress_callback:
                        progress_callback(f"Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ·ªü trang {page_num}")
                    break
                
                if progress_callback:
                    progress_callback(f"T√¨m th·∫•y {len(product_links)} s·∫£n ph·∫©m ·ªü trang {page_num}")
                
                # Scrape each product
                page_products = []
                for i, product_url in enumerate(product_links, 1):  # Scrape ALL products on page
                    if progress_callback:
                        progress_callback(f"Trang {page_num}: Scraping s·∫£n ph·∫©m {i}/{len(product_links)}")
                    
                    try:
                        # Use the base scraper to get product details
                        product_data = self.base_scraper.scrape_product(product_url)
                        
                        if 'error' not in product_data:
                            product_data['page_number'] = page_num
                            product_data['position_on_page'] = i
                            page_products.append(product_data)
                            total_scraped += 1
                        
                        # Add delay between products
                        time.sleep(random.uniform(1, 2))
                        
                    except Exception as e:
                        if progress_callback:
                            progress_callback(f"L·ªói scraping {product_url}: {str(e)}")
                        continue
                
                all_products.extend(page_products)
                
                if progress_callback:
                    progress_callback(f"Ho√†n th√†nh trang {page_num}: {len(page_products)} s·∫£n ph·∫©m")
            
            # Prepare final result
            result = {
                'search_url': search_url,
                'total_pages_scraped': max_pages,
                'total_products': len(all_products),
                'products': all_products,
                'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'summary': {
                    'pages_processed': page_num,
                    'products_found': total_scraped,
                    'success_rate': f"{(total_scraped/max(len(product_links)*max_pages, 1)*100):.1f}%" if product_links else "0%"
                }
            }
            
            return result
            
        except requests.exceptions.RequestException as e:
            return {
                'error': f'Network error: {str(e)}'
            }
        except Exception as e:
            return {
                'error': f'Scraping error: {str(e)}'
            }

# ===================================================================
# GUI INTERFACE CLASS
# ===================================================================

class AmazonScraperGUI:
    def __init__(self, root):
        self.root = root
        self.scraper = AmazonScraper()
        self.search_scraper = AmazonSearchScraper()
        self.current_result = None
        
        # Configure main window
        self.root.title("Amazon Product Scraper - GUI Edition")
        self.root.geometry("950x750")
        self.root.configure(bg='#f0f0f0')
        
        # Configure style
        self.setup_styles()
        
        # Create GUI elements
        self.create_widgets()
        
        # Center window
        self.center_window()
        
        # Add welcome message
        self.show_welcome()

    def setup_styles(self):
        """Setup custom styles for the GUI"""
        self.style = ttk.Style()
        self.style.theme_use('clam')
        
        # Configure styles
        self.style.configure('Title.TLabel', 
                           font=('Arial', 18, 'bold'), 
                           background='#f0f0f0',
                           foreground='#2c3e50')
        
        self.style.configure('Header.TLabel', 
                           font=('Arial', 12, 'bold'), 
                           background='#f0f0f0',
                           foreground='#34495e')

    def create_widgets(self):
        """Create all GUI widgets"""
        
        # Main container
        main_frame = ttk.Frame(self.root, padding="25")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # Configure grid weights
        self.root.columnconfigure(0, weight=1)
        self.root.rowconfigure(0, weight=1)
        main_frame.columnconfigure(1, weight=1)
        
        # Title
        title_label = ttk.Label(main_frame, text="üõí Amazon Product Scraper", style='Title.TLabel')
        title_label.grid(row=0, column=0, columnspan=3, pady=(0, 25))
        
        # Subtitle
        subtitle_label = ttk.Label(main_frame, text="Giao di·ªán ƒë·ªì h·ªça cho scraping Amazon - G·ªôp t·∫•t c·∫£ t√≠nh nƒÉng", style='Header.TLabel')
        subtitle_label.grid(row=1, column=0, columnspan=3, pady=(0, 20))
        
        # URL Input Section
        url_frame = ttk.LabelFrame(main_frame, text="üìù Nh·∫≠p URL Amazon", padding="20")
        url_frame.grid(row=2, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=(0, 20))
        url_frame.columnconfigure(0, weight=1)
        
        # Scrape Mode Selection
        mode_frame = ttk.Frame(url_frame)
        mode_frame.grid(row=0, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=(0, 15))
        
        self.scrape_mode = tk.StringVar(value="single")
        single_radio = ttk.Radiobutton(mode_frame, text="üîó Scrape 1 s·∫£n ph·∫©m chi ti·∫øt", 
                                      variable=self.scrape_mode, value="single",
                                      command=self.on_mode_change)
        single_radio.grid(row=0, column=0, padx=(0, 30))
        
        search_radio = ttk.Radiobutton(mode_frame, text="üîç Scrape search results (nhi·ªÅu s·∫£n ph·∫©m)", 
                                      variable=self.scrape_mode, value="search",
                                      command=self.on_mode_change)
        search_radio.grid(row=0, column=1)
        
        # URL Entry
        self.url_var = tk.StringVar()
        self.url_entry = ttk.Entry(url_frame, textvariable=self.url_var, font=('Arial', 11), width=60)
        self.url_entry.grid(row=1, column=0, sticky=(tk.W, tk.E), padx=(0, 15))
        
        # Pages input (for search mode)
        self.pages_var = tk.StringVar(value="1")
        self.pages_label = ttk.Label(url_frame, text="S·ªë trang:")
        self.pages_entry = ttk.Entry(url_frame, textvariable=self.pages_var, width=8)
        
        # Scrape Button
        self.scrape_button = ttk.Button(url_frame, text="üîç B·∫ÆT ƒê·∫¶U SCRAPE", command=self.start_scraping)
        self.scrape_button.grid(row=1, column=1)
        
        # Example URL
        self.example_label = ttk.Label(url_frame, text="V√≠ d·ª•: https://www.amazon.com/dp/B08N5WRWNW", 
                                     foreground='#7f8c8d')
        self.example_label.grid(row=2, column=0, columnspan=3, sticky=tk.W, pady=(8, 0))
        
        # Progress Section
        progress_frame = ttk.Frame(main_frame)
        progress_frame.grid(row=3, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=(0, 20))
        progress_frame.columnconfigure(0, weight=1)
        
        # Progress Bar
        self.progress = ttk.Progressbar(progress_frame, mode='indeterminate')
        self.progress.grid(row=0, column=0, sticky=(tk.W, tk.E), padx=(0, 15))
        
        # Status Label
        self.status_var = tk.StringVar(value="‚úÖ S·∫µn s√†ng scrape s·∫£n ph·∫©m Amazon")
        self.status_label = ttk.Label(progress_frame, textvariable=self.status_var, font=('Arial', 10))
        self.status_label.grid(row=0, column=1)
        
        # Results Section
        results_frame = ttk.LabelFrame(main_frame, text="üìä K·∫øt qu·∫£ Scraping", padding="20")
        results_frame.grid(row=4, column=0, columnspan=3, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 20))
        results_frame.columnconfigure(0, weight=1)
        results_frame.rowconfigure(0, weight=1)
        
        # Results Text Area
        self.results_text = scrolledtext.ScrolledText(results_frame, height=18, font=('Consolas', 9), wrap=tk.WORD)
        self.results_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # Buttons Section
        buttons_frame = ttk.Frame(main_frame)
        buttons_frame.grid(row=5, column=0, columnspan=3, pady=(15, 0))
        
        # Clear Button
        self.clear_button = ttk.Button(buttons_frame, text="üóëÔ∏è X√≥a k·∫øt qu·∫£", command=self.clear_results)
        self.clear_button.grid(row=0, column=0, padx=(0, 15))
        
        # Save Button
        self.save_button = ttk.Button(buttons_frame, text="üíæ L∆∞u JSON", command=self.save_results, state='disabled')
        self.save_button.grid(row=0, column=1, padx=(0, 15))
        
        # Open Browser Button
        self.browser_button = ttk.Button(buttons_frame, text="üåê M·ªü tr√¨nh duy·ªát", command=self.open_in_browser, state='disabled')
        self.browser_button.grid(row=0, column=2, padx=(0, 15))
        
        # About Button
        self.about_button = ttk.Button(buttons_frame, text="‚ÑπÔ∏è V·ªÅ ch∆∞∆°ng tr√¨nh", command=self.show_about)
        self.about_button.grid(row=0, column=3)
        
        # Configure main frame row weights
        main_frame.rowconfigure(4, weight=1)

    def center_window(self):
        """Center the window on screen"""
        self.root.update_idletasks()
        width = self.root.winfo_width()
        height = self.root.winfo_height()
        x = (self.root.winfo_screenwidth() // 2) - (width // 2)
        y = (self.root.winfo_screenheight() // 2) - (height // 2)
        self.root.geometry(f'{width}x{height}+{x}+{y}')

    def show_welcome(self):
        """Show welcome message"""
        welcome_text = """üéâ CH√ÄO M·ª™NG ƒê√âN V·ªöI AMAZON SCRAPER GUI!

‚ú® Ch∆∞∆°ng tr√¨nh n√†y g·ªôp t·∫•t c·∫£ t√≠nh nƒÉng scraping Amazon v√†o m·ªôt giao di·ªán ƒë·ªì h·ªça duy nh·∫•t:

üîó SINGLE PRODUCT MODE:
   ‚Ä¢ Scrape th√¥ng tin chi ti·∫øt 1 s·∫£n ph·∫©m Amazon
   ‚Ä¢ Bao g·ªìm: gi√°, ƒë√°nh gi√°, h√¨nh ·∫£nh, th√¥ng s·ªë k·ªπ thu·∫≠t, v.v.

üîç SEARCH RESULTS MODE:
   ‚Ä¢ Scrape nhi·ªÅu s·∫£n ph·∫©m t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm
   ‚Ä¢ H·ªó tr·ª£ scrape nhi·ªÅu trang (1-5 trang)
   ‚Ä¢ Th·ªëng k√™ t·ªïng h·ª£p

üìã H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG:
   1. Ch·ªçn ch·∫ø ƒë·ªô scraping (Single product ho·∫∑c Search results)
   2. Nh·∫≠p URL Amazon h·ª£p l·ªá
   3. Nh·∫•n "B·∫ÆT ƒê·∫¶U SCRAPE"
   4. Xem k·∫øt qu·∫£ v√† l∆∞u file JSON

‚ö†Ô∏è L∆ØU √ù:
   ‚Ä¢ Ch·ªâ s·ª≠ d·ª•ng cho m·ª•c ƒë√≠ch h·ªçc t·∫≠p
   ‚Ä¢ T√¥n tr·ªçng robots.txt c·ªßa Amazon
   ‚Ä¢ Kh√¥ng spam requests

üöÄ H√£y b·∫Øt ƒë·∫ßu b·∫±ng c√°ch nh·∫≠p URL Amazon v√† ch·ªçn ch·∫ø ƒë·ªô scraping!"""
        
        self.results_text.insert(1.0, welcome_text)

    def on_mode_change(self):
        """Handle scrape mode change"""
        mode = self.scrape_mode.get()
        
        if mode == "single":
            # Hide pages input
            self.pages_label.grid_remove()
            self.pages_entry.grid_remove()
            self.scrape_button.grid(row=1, column=1)
            self.example_label.config(text="V√≠ d·ª•: https://www.amazon.com/dp/B08N5WRWNW")
            
        else:  # search mode
            # Show pages input
            self.pages_label.grid(row=1, column=1, padx=(15, 8))
            self.pages_entry.grid(row=1, column=2, padx=(0, 15))
            self.scrape_button.grid(row=1, column=3)
            self.example_label.config(text="V√≠ d·ª•: https://www.amazon.com/s?k=cleaning+tools")

    def start_scraping(self):
        """Start scraping in a separate thread"""
        url = self.url_var.get().strip()
        mode = self.scrape_mode.get()
        
        if not url:
            messagebox.showwarning("‚ö†Ô∏è C·∫£nh b√°o", "Vui l√≤ng nh·∫≠p URL Amazon!")
            return
        
        # Get number of pages for search mode
        max_pages = 1
        if mode == "search":
            try:
                pages_str = self.pages_var.get().strip()
                if pages_str:
                    max_pages = int(pages_str)
                    if max_pages < 1 or max_pages > 5:
                        messagebox.showwarning("‚ö†Ô∏è C·∫£nh b√°o", "S·ªë trang ph·∫£i t·ª´ 1-5!")
                        return
            except ValueError:
                messagebox.showwarning("‚ö†Ô∏è C·∫£nh b√°o", "S·ªë trang ph·∫£i l√† s·ªë nguy√™n!")
                return
        
        # Disable button and start progress
        self.scrape_button.config(state='disabled', text="‚è≥ ƒêang scrape...")
        self.progress.start(10)
        
        if mode == "single":
            self.status_var.set("üîÑ ƒêang scrape s·∫£n ph·∫©m... Vui l√≤ng ƒë·ª£i...")
            # Start single product scraping
            thread = threading.Thread(target=self.scrape_product, args=(url,))
        else:
            self.status_var.set(f"üîÑ ƒêang scrape search results ({max_pages} trang)... Vui l√≤ng ƒë·ª£i...")
            # Start search results scraping
            thread = threading.Thread(target=self.scrape_search_results, args=(url, max_pages))
        
        thread.daemon = True
        thread.start()

    def scrape_product(self, url):
        """Scrape product in background thread"""
        try:
            result = self.scraper.scrape_product(url)
            
            # Update GUI in main thread
            self.root.after(0, self.on_scrape_complete, result)
            
        except Exception as e:
            error_result = {'error': f'Unexpected error: {str(e)}'}
            self.root.after(0, self.on_scrape_complete, error_result)

    def scrape_search_results(self, url, max_pages):
        """Scrape search results in background thread"""
        try:
            def progress_callback(message):
                self.root.after(0, lambda: self.status_var.set(f"üîÑ {message}"))
            
            result = self.search_scraper.scrape_search_results(url, max_pages, progress_callback)
            
            # Update GUI in main thread
            self.root.after(0, self.on_scrape_complete, result)
            
        except Exception as e:
            error_result = {'error': f'Unexpected error: {str(e)}'}
            self.root.after(0, self.on_scrape_complete, error_result)

    def on_scrape_complete(self, result):
        """Handle scraping completion"""
        # Stop progress and re-enable button
        self.progress.stop()
        self.scrape_button.config(state='normal', text="üîç B·∫ÆT ƒê·∫¶U SCRAPE")
        
        # Store result
        self.current_result = result
        
        # Display results
        if 'error' in result:
            self.status_var.set(f"‚ùå L·ªói: {result['error']}")
            self.display_error(result['error'])
            self.save_button.config(state='disabled')
            self.browser_button.config(state='disabled')
        else:
            # Check if it's search results or single product
            if 'products' in result:
                self.status_var.set(f"‚úÖ Scrape th√†nh c√¥ng! {result['total_products']} s·∫£n ph·∫©m ƒë∆∞·ª£c t√¨m th·∫•y")
                self.display_search_results(result)
            else:
                self.status_var.set("‚úÖ Scrape th√†nh c√¥ng! S·∫£n ph·∫©m ƒë√£ ƒë∆∞·ª£c ph√¢n t√≠ch")
                self.display_results(result)
            self.save_button.config(state='normal')
            self.browser_button.config(state='normal')

    def display_results(self, result):
        """Display single product scraping results"""
        self.results_text.delete(1.0, tk.END)
        
        # Format results nicely
        output = "="*70 + "\n"
        output += "üéâ TH√îNG TIN S·∫¢N PH·∫®M AMAZON - SCRAPE TH√ÄNH C√îNG!\n"
        output += "="*70 + "\n\n"
        
        if 'title' in result:
            output += f"üì¶ T√äN S·∫¢N PH·∫®M:\n{result['title']}\n\n"
        
        if 'price' in result:
            output += f"üí∞ GI√Å: {result['price']}\n\n"
        
        if 'rating' in result:
            output += f"‚≠ê ƒê√ÅNH GI√Å: {result['rating']}/5"
            if 'review_count' in result:
                output += f" ({result['review_count']} reviews)"
            output += "\n\n"
        
        if 'availability' in result:
            output += f"üì¶ T√åNH TR·∫†NG: {result['availability']}\n\n"
        
        if 'asin' in result:
            output += f"üè∑Ô∏è ASIN: {result['asin']}\n\n"
        
        if 'brand' in result:
            output += f"üè¢ TH∆Ø∆†NG HI·ªÜU: {result['brand']}\n\n"
        
        if 'color' in result:
            output += f"üé® M√ÄU S·∫ÆC: {result['color']}\n\n"
        
        if 'material' in result:
            output += f"üß± CH·∫§T LI·ªÜU: {result['material']}\n\n"
        
        if 'dimensions' in result:
            output += f"üìè K√çCH TH∆Ø·ªöC: {result['dimensions']}\n\n"
        
        if 'weight' in result:
            output += f"‚öñÔ∏è TR·ªåNG L∆Ø·ª¢NG: {result['weight']}\n\n"
        
        if 'model_number' in result:
            output += f"üî¢ M√É MODEL: {result['model_number']}\n\n"
        
        if 'primary_category' in result:
            output += f"üìÇ DANH M·ª§C: {result['primary_category']}\n\n"
        
        if 'prime_eligible' in result:
            prime_status = "‚úÖ C√≥" if result['prime_eligible'] else "‚ùå Kh√¥ng"
            output += f"‚ö° AMAZON PRIME: {prime_status}\n\n"
        
        if 'bestsellers_rank' in result:
            output += f"üèÜ X·∫æP H·∫†NG BESTSELLER: {result['bestsellers_rank']}\n\n"
        
        if 'seller' in result:
            output += f"üè™ NG∆Ø·ªúI B√ÅN: {result['seller']}\n\n"
        
        if 'shipping_info' in result:
            output += f"üöö TH√îNG TIN V·∫¨N CHUY·ªÇN: {result['shipping_info']}\n\n"
        
        if 'variations' in result and result['variations']:
            output += "üéõÔ∏è C√ÅC T√ôY CH·ªåN:\n"
            if 'colors' in result['variations']:
                output += f"   üé® M√†u s·∫Øc c√≥ s·∫µn: {', '.join(result['variations']['colors'][:8])}\n"
            if 'sizes' in result['variations']:
                output += f"   üìè K√≠ch c·ª° c√≥ s·∫µn: {', '.join(result['variations']['sizes'][:8])}\n"
            output += "\n"
        
        if 'features' in result and result['features']:
            output += "üîç ƒê·∫∂C ƒêI·ªÇM CH√çNH:\n"
            for i, feature in enumerate(result['features'][:7], 1):
                feature_clean = feature[:150] + "..." if len(feature) > 150 else feature
                output += f"   {i}. {feature_clean}\n"
            output += "\n"
        
        if 'specifications' in result and result['specifications']:
            output += "üìã TH√îNG S·ªê K·ª∏ THU·∫¨T:\n"
            spec_count = 0
            for key, value in result['specifications'].items():
                if spec_count < 12:  # Show up to 12 specs
                    value_clean = value[:100] + "..." if len(value) > 100 else value
                    output += f"   ‚Ä¢ {key}: {value_clean}\n"
                    spec_count += 1
            if len(result['specifications']) > 12:
                output += f"   ... v√† {len(result['specifications']) - 12} th√¥ng s·ªë kh√°c\n"
            output += "\n"
        
        if 'detailed_description' in result and result['detailed_description']:
            output += "üìù M√î T·∫¢ CHI TI·∫æT:\n"
            for i, desc in enumerate(result['detailed_description'][:3], 1):
                desc_short = desc[:250] + "..." if len(desc) > 250 else desc
                output += f"   {i}. {desc_short}\n"
            output += "\n"
        
        if 'categories' in result and result['categories']:
            output += f"üìÅ DANH M·ª§C ƒê·∫¶Y ƒê·ª¶:\n   {' > '.join(result['categories'])}\n\n"
        
        if 'images' in result and result['images']:
            output += f"üñºÔ∏è H√åNH ·∫¢NH S·∫¢N PH·∫®M ({len(result['images'])} ·∫£nh):\n"
            for i, img_url in enumerate(result['images'][:5], 1):
                output += f"   {i}. {img_url}\n"
            if len(result['images']) > 5:
                output += f"   ... v√† {len(result['images']) - 5} ·∫£nh kh√°c\n"
            output += "\n"
        
        if 'url' in result:
            output += f"üîó URL G·ªêC: {result['url']}\n\n"
        
        if 'scraped_at' in result:
            output += f"‚è∞ TH·ªúI GIAN SCRAPE: {result['scraped_at']}\n\n"
        
        output += "="*70 + "\n"
        output += "üíæ D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng ƒë·ªÉ l∆∞u th√†nh file JSON!\n"
        output += "üåê C√≥ th·ªÉ m·ªü URL trong tr√¨nh duy·ªát ƒë·ªÉ xem s·∫£n ph·∫©m!\n"
        output += "üîÑ Th·ª≠ scrape s·∫£n ph·∫©m kh√°c ho·∫∑c search results!\n"
        
        self.results_text.insert(1.0, output)

    def display_search_results(self, result):
        """Display search results"""
        self.results_text.delete(1.0, tk.END)
        
        # Format search results nicely
        output = "="*70 + "\n"
        output += "üéâ K·∫æT QU·∫¢ SCRAPING SEARCH RESULTS - TH√ÄNH C√îNG!\n"
        output += "="*70 + "\n\n"
        
        output += f"üîó URL t√¨m ki·∫øm: {result['search_url']}\n"
        output += f"üìÑ S·ªë trang ƒë√£ scrape: {result['total_pages_scraped']}\n"
        output += f"üì¶ T·ªïng s·ªë s·∫£n ph·∫©m t√¨m th·∫•y: {result['total_products']}\n"
        output += f"‚è∞ Th·ªùi gian scrape: {result['scraped_at']}\n"
        
        if 'summary' in result:
            output += f"üìä T·ª∑ l·ªá scrape th√†nh c√¥ng: {result['summary']['success_rate']}\n"
        
        output += "\n" + "="*70 + "\n"
        output += "üìã DANH S√ÅCH S·∫¢N PH·∫®M\n"
        output += "="*70 + "\n\n"
        
        # Display products
        for i, product in enumerate(result['products'][:25], 1):  # Show first 25
            output += f"üõçÔ∏è S·∫¢N PH·∫®M {i}:\n"
            
            if 'title' in product:
                title = product['title'][:90] + "..." if len(product['title']) > 90 else product['title']
                output += f"   üì¶ {title}\n"
            
            # Product details in compact format
            details = []
            if 'price' in product:
                details.append(f"üí∞ {product['price']}")
            if 'rating' in product:
                details.append(f"‚≠ê {product['rating']}/5")
            if 'review_count' in product:
                details.append(f"üìù {product['review_count']} reviews")
            if 'page_number' in product:
                details.append(f"üìÑ Trang {product['page_number']}")
            if 'position_on_page' in product:
                details.append(f"üî¢ V·ªã tr√≠ {product['position_on_page']}")
            
            if details:
                output += f"   {' | '.join(details)}\n"
            
            if 'availability' in product:
                output += f"   üì¶ T√¨nh tr·∫°ng: {product['availability']}\n"
            
            if 'brand' in product:
                output += f"   üè¢ Th∆∞∆°ng hi·ªáu: {product['brand']}\n"
            
            if 'url' in product:
                output += f"   üîó {product['url']}\n"
            
            output += "\n"
        
        if len(result['products']) > 25:
            output += f"... v√† {len(result['products']) - 25} s·∫£n ph·∫©m kh√°c (xem trong file JSON)\n\n"
        
        output += "="*70 + "\n"
        output += "üíæ D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng ƒë·ªÉ l∆∞u th√†nh file JSON!\n"
        output += "üåê C√≥ th·ªÉ m·ªü search URL trong tr√¨nh duy·ªát!\n"
        output += "üîÑ Th·ª≠ v·ªõi t·ª´ kh√≥a t√¨m ki·∫øm kh√°c ho·∫∑c scrape single product!\n"
        
        self.results_text.insert(1.0, output)

    def display_error(self, error_message):
        """Display error message"""
        self.results_text.delete(1.0, tk.END)
        
        output = "="*70 + "\n"
        output += "‚ùå L·ªñI SCRAPING\n"
        output += "="*70 + "\n\n"
        output += f"‚ùó Chi ti·∫øt l·ªói: {error_message}\n\n"
        output += "üîß G·ª¢I √ù KH·∫ÆC PH·ª§C:\n"
        output += "‚Ä¢ ‚úÖ Ki·ªÉm tra URL c√≥ ƒë√∫ng ƒë·ªãnh d·∫°ng Amazon kh√¥ng\n"
        output += "‚Ä¢ ‚úÖ ƒê·∫£m b·∫£o k·∫øt n·ªëi internet ·ªïn ƒë·ªãnh\n"
        output += "‚Ä¢ ‚úÖ Th·ª≠ l·∫°i sau v√†i ph√∫t n·∫øu b·ªã rate limit\n"
        output += "‚Ä¢ ‚úÖ S·ª≠ d·ª•ng URL s·∫£n ph·∫©m Amazon h·ª£p l·ªá\n"
        output += "‚Ä¢ ‚úÖ Th·ª≠ v·ªõi URL kh√°c ho·∫∑c ch·∫ø ƒë·ªô kh√°c\n\n"
        output += "üìã V√ç D·ª§ URL H·ª¢P L·ªÜ:\n"
        output += "üîó Single Product:\n"
        output += "   ‚Ä¢ https://www.amazon.com/dp/B08N5WRWNW\n"
        output += "   ‚Ä¢ https://www.amazon.com/gp/product/B08N5WRWNW\n\n"
        output += "üîç Search Results:\n"
        output += "   ‚Ä¢ https://www.amazon.com/s?k=cleaning+tools\n"
        output += "   ‚Ä¢ https://www.amazon.com/s?k=wireless+headphones\n\n"
        output += "üí° M·∫∏O: H√£y th·ª≠ copy URL tr·ª±c ti·∫øp t·ª´ thanh ƒë·ªãa ch·ªâ tr√¨nh duy·ªát!\n"
        
        self.results_text.insert(1.0, output)

    def clear_results(self):
        """Clear results text area"""
        self.results_text.delete(1.0, tk.END)
        self.show_welcome()
        self.status_var.set("‚úÖ ƒê√£ x√≥a k·∫øt qu·∫£. S·∫µn s√†ng scrape s·∫£n ph·∫©m m·ªõi.")
        self.current_result = None
        self.save_button.config(state='disabled')
        self.browser_button.config(state='disabled')

    def save_results(self):
        """Save results to JSON file"""
        if not self.current_result:
            messagebox.showwarning("‚ö†Ô∏è C·∫£nh b√°o", "Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u!")
            return
        
        # Generate filename based on result type
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if 'products' in self.current_result:
            # Search results
            default_filename = f"amazon_search_results_{timestamp}.json"
        else:
            # Single product
            default_filename = f"amazon_product_{timestamp}.json"
        
        # Ask user for save location
        filename = filedialog.asksaveasfilename(
            defaultextension=".json",
            filetypes=[("JSON files", "*.json"), ("All files", "*.*")],
            initialfile=default_filename,
            title="L∆∞u k·∫øt qu·∫£ scraping Amazon"
        )
        
        if filename:
            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(self.current_result, f, ensure_ascii=False, indent=2)
                
                # Show success message with file info
                file_size = os.path.getsize(filename) / 1024  # KB
                product_count = ""
                if 'products' in self.current_result:
                    product_count = f" ({self.current_result['total_products']} s·∫£n ph·∫©m)"
                
                messagebox.showinfo("‚úÖ Th√†nh c√¥ng", 
                                  f"ƒê√£ l∆∞u d·ªØ li·ªáu{product_count} v√†o:\n{filename}\n\nK√≠ch th∆∞·ªõc file: {file_size:.1f} KB")
                self.status_var.set(f"‚úÖ ƒê√£ l∆∞u: {os.path.basename(filename)} ({file_size:.1f} KB)")
                
            except Exception as e:
                messagebox.showerror("‚ùå L·ªói", f"Kh√¥ng th·ªÉ l∆∞u file:\n{str(e)}")

    def open_in_browser(self):
        """Open the scraped product URL in browser"""
        if not self.current_result:
            messagebox.showwarning("‚ö†Ô∏è C·∫£nh b√°o", "Kh√¥ng c√≥ URL ƒë·ªÉ m·ªü!")
            return
        
        try:
            # Check if it's search results or single product
            if 'products' in self.current_result:
                # For search results, open the search URL
                url = self.current_result.get('search_url')
                url_type = "search URL"
            else:
                # For single product, open the product URL
                url = self.current_result.get('url')
                url_type = "product URL"
            
            if not url:
                messagebox.showwarning("‚ö†Ô∏è C·∫£nh b√°o", "Kh√¥ng c√≥ URL ƒë·ªÉ m·ªü!")
                return
            
            webbrowser.open(url)
            self.status_var.set(f"üåê ƒê√£ m·ªü {url_type} trong tr√¨nh duy·ªát")
            
        except Exception as e:
            messagebox.showerror("‚ùå L·ªói", f"Kh√¥ng th·ªÉ m·ªü tr√¨nh duy·ªát:\n{str(e)}")

    def show_about(self):
        """Show about dialog"""
        about_text = """üõí Amazon Product Scraper - GUI Edition v1.0

Ch∆∞∆°ng tr√¨nh scrape th√¥ng tin s·∫£n ph·∫©m Amazon 
v·ªõi giao di·ªán ƒë·ªì h·ªça th√¢n thi·ªán - G·ªôp t·∫•t c·∫£ t√≠nh nƒÉng!

‚ú® T√çNH NƒÇNG:
‚Ä¢ üîó Scrape th√¥ng tin chi ti·∫øt s·∫£n ph·∫©m Amazon
‚Ä¢ üîç Scrape nhi·ªÅu s·∫£n ph·∫©m t·ª´ search results  
‚Ä¢ üñ•Ô∏è Giao di·ªán ƒë·ªì h·ªça ƒë·∫ßy ƒë·ªß v·ªõi tkinter
‚Ä¢ üíæ L∆∞u k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON
‚Ä¢ üåê M·ªü s·∫£n ph·∫©m trong tr√¨nh duy·ªát
‚Ä¢ üåç H·ªó tr·ª£ nhi·ªÅu domain Amazon (.com, .co.uk, .de, etc.)
‚Ä¢ üìä Theo d√µi ti·∫øn ƒë·ªô scraping
‚Ä¢ ‚ö° Rate limiting t·ª± ƒë·ªông
‚Ä¢ üõ°Ô∏è T·∫•t c·∫£ trong 1 file duy nh·∫•t!

üéØ TH√îNG TIN ƒê∆Ø·ª¢C SCRAPE:
‚Ä¢ T√™n, gi√°, ƒë√°nh gi√°, reviews
‚Ä¢ ASIN, th∆∞∆°ng hi·ªáu, m√†u s·∫Øc, ch·∫•t li·ªáu  
‚Ä¢ K√≠ch th∆∞·ªõc, tr·ªçng l∆∞·ª£ng, model
‚Ä¢ T√¨nh tr·∫°ng, shipping, seller
‚Ä¢ ƒê·∫∑c ƒëi·ªÉm, m√¥ t·∫£, th√¥ng s·ªë k·ªπ thu·∫≠t
‚Ä¢ H√¨nh ·∫£nh, danh m·ª•c, variations
‚Ä¢ Prime eligibility, bestseller rank

‚ö†Ô∏è L∆ØU √ù:
‚Ä¢ Ch·ªâ s·ª≠ d·ª•ng cho m·ª•c ƒë√≠ch h·ªçc t·∫≠p
‚Ä¢ T√¥n tr·ªçng robots.txt v√† ToS c·ªßa Amazon
‚Ä¢ Kh√¥ng spam requests
‚Ä¢ Th√™m delay h·ª£p l√Ω gi·ªØa requests

üîß Ph√°t tri·ªÉn b·ªüi: AI Assistant
üìÖ NƒÉm: 2024

üåü Enjoy scraping responsibly! üåü"""
        
        messagebox.showinfo("‚ÑπÔ∏è V·ªÅ ch∆∞∆°ng tr√¨nh", about_text)

# ===================================================================
# MAIN FUNCTION - CH·ªà GUI
# ===================================================================

def main():
    """Main function - GUI only version"""
    # Check dependencies
    try:
        print("üîç ƒêang ki·ªÉm tra dependencies...")
        import requests
        import bs4
        print("‚úÖ Dependencies check: OK")
    except ImportError as e:
        print(f"‚ùå Thi·∫øu th∆∞ vi·ªán c·∫ßn thi·∫øt: {e}")
        print("üì• C√†i ƒë·∫∑t b·∫±ng l·ªánh:")
        print("   pip install requests beautifulsoup4 lxml")
        messagebox.showerror("‚ùå L·ªói Dependencies", 
                           f"Thi·∫øu th∆∞ vi·ªán: {e}\n\nH√£y ch·∫°y l·ªánh:\npip install requests beautifulsoup4 lxml")
        sys.exit(1)
    
    print("üöÄ Kh·ªüi ƒë·ªông Amazon Scraper GUI...")
    
    # Create and run GUI
    try:
        root = tk.Tk()
        app = AmazonScraperGUI(root)
        
        # Handle window closing
        def on_closing():
            if messagebox.askokcancel("üö™ Tho√°t", "B·∫°n c√≥ mu·ªën tho√°t Amazon Scraper?"):
                print("üëã ƒê√£ tho√°t Amazon Scraper GUI.")
                root.destroy()
        
        root.protocol("WM_DELETE_WINDOW", on_closing)
        
        print("‚úÖ GUI kh·ªüi ƒë·ªông th√†nh c√¥ng!")
        print("üìã H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng c√≥ trong giao di·ªán.")
        
        # Start GUI main loop
        root.mainloop()
        
    except Exception as e:
        error_msg = f"Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông GUI: {e}"
        print(f"‚ùå {error_msg}")
        messagebox.showerror("‚ùå L·ªói GUI", error_msg)
        sys.exit(1)

if __name__ == "__main__":
    main() 